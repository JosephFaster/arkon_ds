{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelos y entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librerías esenciales\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Modelos y evaluación\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import lightgbm as lgb\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Visualización\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Configurar estilo de gráficos\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "# import optuna\n",
    "# from sklearn.model_selection import StratifiedKFold\n",
    "# from sklearn.metrics import f1_score\n",
    "\n",
    "print(\"Librerías importadas correctamente.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir rutas de los datasets procesados\n",
    "train_path = \"../data/df_train_processed.feather\"\n",
    "test_path = \"../data/df_test_processed.feather\"\n",
    "\n",
    "# Cargar los datasets\n",
    "df_train = pd.read_feather(train_path)\n",
    "df_test = pd.read_feather(test_path)\n",
    "\n",
    "# Mostrar las dimensiones de los datos cargados\n",
    "print(f\"Datos cargados correctamente.\")\n",
    "print(f\"Dimensiones - Train: {df_train.shape}, Test: {df_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.info()\n",
    "df_test.info()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Al guardarlos en formato feather hubo un detalle con datos object, por lo que se deben convertir a categoricos\n",
    "df_train[\"year_month\"] = df_train[\"year_month\"].astype(\"category\")\n",
    "df_test[\"year_month\"] = df_test[\"year_month\"].astype(\"category\")\n",
    "df_train.info()\n",
    "df_test.info()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento de modelos\n",
    "\n",
    "Necesitamos dividir los datos en tres conjuntos:\n",
    "1. **Train (70%)** Se usa para entrenar el modelo.  \n",
    "2. **Validation (15%)**  Se usa para ajustar hiperparámetros y evaluar el desempeño durante el entrenamiento.  \n",
    "3. **Test (15%)** Se usa para evaluar el rendimiento final antes de usar el modelo en producción.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Definir variables predictoras y objetivo\n",
    "X = df_train.drop(columns=[\"passholder_type\"])  # Todas menos la variable objetivo\n",
    "y = df_train[\"passholder_type\"]  # Variable a predecir\n",
    "\n",
    "# Primero separamos 70% train y 30% restante\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.30, random_state=42, stratify=y)\n",
    "\n",
    "# Ahora dividimos el 30% restante en validación (15%) y test (15%)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.50, random_state=42, stratify=y_temp)\n",
    "\n",
    "# Verificar dimensiones\n",
    "print(f\"Dimensiones después del split:\")\n",
    "print(f\"Train: X_train {X_train.shape}, y_train {y_train.shape}\")\n",
    "print(f\"Validation: X_val {X_val.shape}, y_val {y_val.shape}\")\n",
    "print(f\"Test: X_test {X_test.shape}, y_test {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar el LabelEncoder y ajustar con los valores de la variable objetivo\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_val_encoded = label_encoder.transform(y_val)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "# Convertir `y_train`, `y_val`, y `y_test` en NumPy arrays para evitar errores\n",
    "y_train = y_train_encoded\n",
    "y_val = y_val_encoded\n",
    "y_test = y_test_encoded\n",
    "\n",
    "# Verificar las clases codificadas\n",
    "print(\"Clases codificadas:\", label_encoder.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir los datasets a formato LightGBM\n",
    "train_data = lgb.Dataset(X_train, label=y_train)\n",
    "val_data = lgb.Dataset(X_val, label=y_val)\n",
    "\n",
    "# Definir hiperparámetros del modelo\n",
    "params = {\n",
    "    \"objective\": \"multiclass\",\n",
    "    \"num_class\": len(label_encoder.classes_),\n",
    "    \"boosting_type\": \"gbdt\",\n",
    "    \"metric\": \"multi_logloss\",\n",
    "    \"learning_rate\": 0.07,\n",
    "    \"max_depth\": 13,\n",
    "    \"num_leaves\": 43,\n",
    "    \"min_data_in_leaf\": 30,\n",
    "    \"feature_fraction\": 0.8,\n",
    "    \"bagging_fraction\": 0.8,\n",
    "    \"bagging_freq\": 5,\n",
    "    \"random_state\": 42,\n",
    "    \"verbose\": -1\n",
    "}\n",
    "\n",
    "# Entrenar modelo sin `early_stopping_rounds`\n",
    "print(\"Entrenando LightGBM...\")\n",
    "lgb_model = lgb.train(\n",
    "    params,\n",
    "    train_data,\n",
    "    valid_sets=[val_data],\n",
    "    num_boost_round=500\n",
    ")\n",
    "\n",
    "# Predicciones en validación\n",
    "y_val_pred_proba = lgb_model.predict(X_val)\n",
    "\n",
    "# Convertir probabilidades a clases\n",
    "y_val_pred = y_val_pred_proba.argmax(axis=1)\n",
    "\n",
    "# Evaluar desempeño\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "accuracy = accuracy_score(y_val, y_val_pred)\n",
    "print(f\"Accuracy en validación: {accuracy:.4f}\")\n",
    "\n",
    "print(\"\\nReporte de Clasificación:\")\n",
    "print(classification_report(y_val, y_val_pred, target_names=label_encoder.classes_))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El modelo LightGBM tiene un **accuracy de 74.14%**, lo cual no está mal, pero hay puntos clave que podemos mejorar\n",
    "\n",
    "1. **`Monthly Pass`** es la clase mejor predicha con **f1-score de 0.83**, lo que indica que el modelo captura bien su patrón.\n",
    "2. **`Annual Pass`, `Flex Pass` y `One Day Pass`** tienen baja precisión y recall, lo que sugiere que el modelo tiene problemas diferenciándolos.\n",
    "3. **`Testing`** es completamente ignorada con un f1-score de **0.00**, lo que indica que el modelo no encuentra ejemplos suficientes para aprender de ella y es logico porque había muy pocos datos, quizá haciendo un balanceo de clases se podría mejorar.\n",
    "4. **`Walk-up`** tiene un desempeño aceptable pero no óptimo.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Guardar el modelo LightGBM\n",
    "with open(\"lightgbm_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(lgb_model, f)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Modelos guardados exitosamente\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "| Modelo | Pros | Contras |\n",
    "|--------|------|---------|\n",
    "| **Random Forest** | Bueno con categóricas, fácil de interpretar | Puede ser pesado si tiene muchas categorías distintas |\n",
    "| **XGBoost** | Eficiente y optimizado para tabulares | Puede ser más lento que otros modelos ligeros |\n",
    "| **LightGBM** | Rápido y eficiente con datos categóricos | Puede ser sensible a datos desbalanceados |\n",
    "| **Regresión Logística** | Sencillo, interpretativo y liviano | Puede no capturar relaciones no lineales |\n",
    "| **Naive Bayes** | Muy rápido en inferencia | Suponiendo independencia de features, lo cual puede no ser realista |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Cargar el modelo LightGBM\n",
    "with open(\"lightgbm_model.pkl\", \"rb\") as f:\n",
    "    lgb_model = pickle.load(f)\n",
    "\n",
    "print(\"Modelos cargados exitosamente.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asegurar que las variables categóricas en df_test tienen las mismas categorías que X_train\n",
    "categorical_columns = [\"trip_route_category\", \"start_station\", \"end_station\", \"day_of_week\", \"year_month\"]\n",
    "\n",
    "for col in categorical_columns:\n",
    "    df_test[col] = df_test[col].astype(\"category\")\n",
    "    df_test[col] = df_test[col].cat.set_categories(X_train[col].cat.categories)\n",
    "\n",
    "print(\"Se han corregido las categorías en df_test para coincidir con X_train.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Hacer predicción asegurándonos de que df_test solo tenga las columnas esperadas\n",
    "X_test_model = df_test[X_train.columns]  # Solo seleccionamos las columnas usadas en entrenamiento\n",
    "\n",
    "# Hacer la predicción\n",
    "y_test_pred_proba = lgb_model.predict(X_test_model)\n",
    "\n",
    "# Convertir probabilidades a etiquetas\n",
    "y_test_pred = y_test_pred_proba.argmax(axis=1)\n",
    "\n",
    "# Convertir etiquetas numéricas a nombres de categorías\n",
    "y_test_pred_labels = label_encoder.inverse_transform(y_test_pred)\n",
    "\n",
    "# Crear el DataFrame final con trip_id y la predicción\n",
    "df_submission = pd.DataFrame({\n",
    "    \"trip_id\": df_test[\"trip_id\"],\n",
    "    \"passholder_type\": y_test_pred_labels\n",
    "})\n",
    "\n",
    "# Guardar en CSV\n",
    "df_submission.to_csv(\"submission_lightgbm.csv\", index=False)\n",
    "\n",
    "print(\"Predicciones guardadas en 'submission_lightgbm.csv'\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arkon_ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
